{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import neptune\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, MiniBatchKMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "PATH = os.getcwd()\n",
    "VERSION = \"Sv5_b\"\n",
    "FILE = \"metadatabase_surrogate_\"+VERSION+\".csv\"\n",
    "LOG_FILE = \"log_13_07_b.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_ini = 1\n",
    "par_fin = 10\n",
    "\n",
    "\n",
    "sil_ini_list = np.arange(0.2, 0.6, 0.05)\n",
    "dbs_max_list = np.arange(0.4, 0.8, 0.1)\n",
    "dist_sil_dbs_list = np.arange(0.15, 0.35, 0.05)\n",
    "\n",
    "seed_list = range(int(par_ini),int(par_fin))\n",
    "distribution_list = [[\"exponential\", 'gumbel', 'normal'], [\"exponential\", 'normal'], ['gumbel', 'normal']]\n",
    "\n",
    "algorithms = [RandomForestRegressor]\n",
    "qtd_arvores = [100,250]\n",
    "threshold_qtd_on_training = 50\n",
    "contamination = [0, 0.05, 0.1, 0.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_4bench = ['attr_conc.mean','attr_conc.sd','attr_ent.mean',\n",
    "    'attr_ent.sd','attr_to_inst','cohesiveness.mean','cohesiveness.sd',\n",
    "    #'cor.mean',#'cor.sd',\n",
    "    'cov.mean',#'cov.sd',\n",
    "    'eigenvalues.mean','eigenvalues.sd',\n",
    "    'inst_to_attr','iq_range.mean','iq_range.sd',\n",
    "    #'kurtosis.mean','kurtosis.sd',\n",
    "    'mad.mean','mad.sd',\n",
    "    #'max.mean','max.sd','mean.mean','mean.sd',\n",
    "    'median.mean',\n",
    "    'median.sd',#'min.mean','min.sd',\n",
    "    'nr_attr','nr_cor_attr','nr_inst','one_itemset.mean',\n",
    "    'one_itemset.sd',\n",
    "    #'range.mean','range.sd',\n",
    "    'sd.mean','sd.sd'\n",
    "    #,'skewness.mean', 'skewness.sd'\n",
    "    ,'sparsity.mean','sparsity.sd','t2','t3','t4','t_mean.mean',\n",
    "    't_mean.sd','two_itemset.mean','two_itemset.sd','var.mean','var.sd',\n",
    "    'wg_dist.mean','wg_dist.sd',\n",
    "    'sil', 'dbs', 'clusters', 'cluster_diff'\n",
    "]\n",
    "datasets_selected_benchmarking = os.listdir(join(PATH,\"benchmarks_dataset\"))\n",
    "\n",
    "features = features_4bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv(f\"{PATH}/{FILE}\")\n",
    "\n",
    "#combined_lists = [sil_ini_list, dbs_max_list, dist_sil_dbs_list, seed_list, distribution_list, algorithms]\n",
    "combined_lists = [sil_ini_list, dbs_max_list, dist_sil_dbs_list, seed_list, distribution_list, algorithms, contamination, qtd_arvores]\n",
    "\n",
    "all_combinations = list(itertools.product(*combined_lists))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_distribution(database_input, dist):\n",
    "  database_input['distribution'] = database_input.apply(lambda row: row.Dataset.split('\\'')[1], axis=1)\n",
    "  return database_input[database_input.distribution.isin(dist)].drop([\"distribution\"], axis=1)\n",
    "\n",
    "def filter_sil_bds(database_input, min_sil, max_dbs):\n",
    "  database_input = database_input[database_input.sil>=min_sil]\n",
    "  database_input = database_input[database_input.dbs<=max_dbs]\n",
    "  return database_input\n",
    "\n",
    "def filter_dist_sil_bds(database_input, dist):\n",
    "  database_input = database_input[(database_input.dbs-database_input.sil) <= dist]\n",
    "  return database_input\n",
    "\n",
    "def filter_samples_isolation(database_input, contamination):\n",
    "    if contamination>0:\n",
    "        iso = IsolationForest(contamination=contamination, random_state=39)\n",
    "        yhat = iso.fit_predict(database_input._get_numeric_data())\n",
    "\n",
    "        mask = yhat != -1\n",
    "        database_input = database_input.iloc[mask, :]\n",
    "    return database_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(model_input, features, datasets_selected_benchmarking):\n",
    "  # remove Sil Dbs Clusterdiff cluster\n",
    "  features_local = features[:-4]\n",
    "\n",
    "  k_set = range(2,25,1)\n",
    "\n",
    "  mypath = f\"{PATH}/benchmarks_dataset\"\n",
    "  onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "  # df_benchmark = pd.read_csv(PATH+\"metadatabase_validation_scaled.csv\")\n",
    "  df_benchmark = pd.read_csv(f\"{PATH}/MetaDataset.csv\")\n",
    "  df_benchmark = df_benchmark.iloc[::4,:]\n",
    "\n",
    "  datasets_selected = datasets_selected_benchmarking\n",
    "\n",
    "  # Extract all available unsupervised measures\n",
    "  print(\"Datasets: \",datasets_selected)\n",
    "  datasets = []\n",
    "  for file in datasets_selected:\n",
    "\n",
    "    #print(\">>>\"+file+\"<<<<\")\n",
    "    #try:\n",
    "    \n",
    "    n_clusters_ideal = int(df_benchmark[df_benchmark[\"Dataset\"] == file][\"clusters\"].head(1).values[0])\n",
    "    #except:\n",
    "    #  print(\"ERROR**********************\")\n",
    "    #  continue\n",
    "    #print(\"n_clusters\", n_clusters_ideal)\n",
    "\n",
    "    X = pd.read_csv(mypath+\"/\"+file)\n",
    "    ft = df_benchmark[df_benchmark.Dataset == file].filter(features_local)\n",
    "    ft = ft[features_local].values\n",
    "\n",
    "    #print(X.isnull().any().head(40))\n",
    "\n",
    "    for rep in k_set:\n",
    "      cluster_algo = [AgglomerativeClustering(n_clusters=rep, metric='euclidean', linkage='ward'),\n",
    "                      KMeans(n_clusters=rep, n_init=\"auto\"),\n",
    "                      KMedoids(n_clusters=rep),\n",
    "                      MiniBatchKMeans(n_clusters=rep, batch_size=10,n_init=\"auto\")\n",
    "                      ]\n",
    "      \n",
    "      for c in cluster_algo:\n",
    "        try:\n",
    "          cluster_labels = c.fit_predict(X)\n",
    "          sil = silhouette_score(X, cluster_labels)\n",
    "          dbs = davies_bouldin_score (X, cluster_labels)\n",
    "          mf = ft[0].tolist()\n",
    "          mf.extend([sil,dbs,rep])\n",
    "          yhat = model_input.predict([mf])\n",
    "          datasets.append([file]+[type(c).__name__]+[sil]+[dbs]+[rep]+[n_clusters_ideal]+[yhat])\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "  return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimizing_logging(model_regressor, features, sil_ini, dbs_max, dist_s_d, seed, dist, qtd, run, progress, contamin, qtd_arvores):\n",
    "  datasets = run_exp(model_regressor, features, datasets_selected_benchmarking)\n",
    "  x = pd.DataFrame(datasets)\n",
    "  x.columns = [\"Dataset\", \"Algorithm\", \"sil\", \"dbs\", \"k_candidate\", \"k_expected\", \"yhat\"]\n",
    "  x['yhat'] = x.apply(lambda row: row.yhat[0], axis=1)\n",
    "  \n",
    "  min_data = pd.DataFrame(x.groupby([\"Dataset\"])[\"yhat\"].min().reset_index())\n",
    "  result = pd.merge(x, min_data, on=\"yhat\").drop_duplicates(subset='Dataset_x', keep=\"first\")\n",
    "\n",
    "  mae = np.round(metrics.mean_absolute_error(result.k_candidate, result.k_expected),2)\n",
    "  print(\"MAE\",mae)\n",
    "  mse = np.round(metrics.mean_squared_error(result.k_candidate, result.k_expected),2)\n",
    "  print(\"MSE\",mse)\n",
    "  rmse = np.round(np.sqrt(mse),2) # or mse**(0.5)\n",
    "  print(\"RMSE\",rmse)\n",
    "  r2 = np.round(metrics.r2_score(result.k_candidate,result.k_expected),2)\n",
    "  print(\"R2\",r2)\n",
    "  acc = np.round(metrics.accuracy_score(result.k_candidate,result.k_expected),2)\n",
    "  print(\"ACC\",acc)\n",
    "\n",
    "  std =  np.round(result.yhat.std(),2)\n",
    "  print(\"Std Deviation\", std)\n",
    "\n",
    "  contamination = np.round(contamin,2)\n",
    "\n",
    "  print(result.groupby(['Dataset_x'])[['k_candidate', 'k_expected']]\n",
    "        .mean())\n",
    "\n",
    "  log = [mae, mse, rmse, r2, acc, std, type(model_regressor).__name__, seed, sil_ini, dbs_max, dist_s_d, dist, qtd, contamination, qtd_arvores]\n",
    "  log = pd.DataFrame(log).T\n",
    "  log.columns = [\"MAE\", \"MSE\", \"RMSE\", \"R2\", \"ACC\",  \"STD all clusterers\", \"algorithm\", \"seed\",  \"sil_ini\", \"dbs_max\", \"distance_s_d\", \"distribution\", \"qtd samples\", \"contamination\", \"qtd_arvores\"]\n",
    "  log.to_csv(f\"{PATH}/{LOG_FILE}\", mode='a', index=False, header=False)\n",
    "\n",
    "  run[\"model_regressor\"] = \"RandomForest\"\n",
    "  run[\"features\"] = features\n",
    "  run[\"sil_ini\"] = sil_ini  # time cost\n",
    "  run[\"dbs_max\"] = dbs_max  # parsimonly?\n",
    "  run[\"dist_s_d\"] = dist_s_d\n",
    "  run[\"seed\"] = seed\n",
    "  run[\"Distribution\"] = dist\n",
    "  run[\"Qtd Samples\"] = qtd\n",
    "  run[\"MAE\"] = mae\n",
    "  run[\"MSE\"] = mse\n",
    "  run[\"RMSE\"] = rmse\n",
    "  run[\"R2\"] = r2\n",
    "  run[\"ACC\"] = acc\n",
    "  run[\"STD all clusterers\"] = std\n",
    "  run[\"Progress\"] = progress\n",
    "  run[\"Contamination\"] = contamination\n",
    "  run[\"qtd_arvores\"] = qtd_arvores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_combinations = all_combinations[:3]\n",
    "for index, combination in enumerate(all_combinations):\n",
    "    run = neptune.init_run(\n",
    "        project=\"MaleLab/SV5ml2dac\",\n",
    "        api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIwODNjNDRiNS02MDM4LTQ2NGEtYWQwMC00OGRhYjcwODc0ZDIifQ==\",\n",
    "    )\n",
    "    df_surrogate = original\n",
    "    progress = (index + 1) / len(all_combinations) * 100\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(f\"Progress: {progress:.2f}%\")\n",
    "    sil_ini, dbs_max, dist_s_d, seed, dist, regressor, contamin, qtd_arvores = combination\n",
    "    \n",
    "    df_surrogate = filtering_distribution(df_surrogate, dist)\n",
    "    df_surrogate = filter_sil_bds(df_surrogate, sil_ini, dbs_max)\n",
    "    df_surrogate = filter_dist_sil_bds(df_surrogate, dist_s_d)\n",
    "\n",
    "    df_surrogate = filter_samples_isolation(df_surrogate, contamin)\n",
    "\n",
    "\n",
    "    if(df_surrogate.shape[0]<threshold_qtd_on_training):\n",
    "        print(\"Invalid Combination: \",df_surrogate.shape )\n",
    "        continue\n",
    "    print(\"Samples\", df_surrogate.shape)\n",
    "\n",
    "    df_surrogate['clusters'] = df_surrogate.apply(lambda row: int(row.Dataset.split('-')[1].replace(\"clusters\",\"\")), axis=1)\n",
    "\n",
    "    df_surrogate = df_surrogate[features]\n",
    "\n",
    "    data = df_surrogate\n",
    "    x_train, y_train = data.values[:, :-1], data.values[:, -1]\n",
    "\n",
    "    model_regressor = regressor(random_state=seed, n_estimators=qtd_arvores, n_jobs=-1)\n",
    "    model_regressor.fit(x_train, y_train)\n",
    "    run[\"name\"] = \"A_\"+str(round(contamin,2))+\"_\"+str(seed)+\"_\"+str(round(sil_ini,2))+\"_\"+str(round(dbs_max,2))+\"_\"+str(round(dist_s_d,2))\n",
    "    minimizing_logging(model_regressor, features, sil_ini, dbs_max, dist_s_d, seed, dist, df_surrogate.shape[0], run, progress, contamin, qtd_arvores)\n",
    "\n",
    "    run.sync()\n",
    "    run.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benchmark = pd.read_csv(f\"{PATH}/MetaDataset.csv\")\n",
    "df_benchmark = df_benchmark.iloc[::4,:]\n",
    "\n",
    "print(df_benchmark[df_benchmark[\"Dataset\"] == \"iono.csv\"][\"clusters\"].head(1).values[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
